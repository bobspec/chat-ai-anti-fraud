import streamlit as st
import logging
# æ¨¡å‹ä¸‹è½½
# from modelscope import AutoTokenizer, AutoModelForCausalLM,snapshot_download
# from modelscope import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.DEBUG)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ InternLM2-Chat-7B é˜²è¯ˆéª—ä¸“å®¶")
st.caption("ğŸš€ A streamlit chatbot powered by InternLM2 QLora")

try:
    logging.debug("Starting model download...")
    model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-1_8b', revision='v1.1.0')
    logging.debug(f"Model downloaded to {model_dir}")
except Exception as e:
    logging.error(f"Error during model download: {e}")
    st.error(f"Error during model download: {e}")


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    try:
        logging.debug("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        logging.debug("Tokenizer loaded successfully")

        logging.debug("Loading model...")
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.debug("Using GPU for model loading")
            model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, torch_dtype=torch.float16)
        else:
            device = torch.device("cpu")
            logging.debug("Using CPU for model loading")
            model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, torch_dtype=torch.float32)

        model = model.to(device)
        model = model.eval()
        logging.debug("Model loaded successfully")
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error during model loading: {e}")
        st.error(f"Error during model loading: {e}")
        return None, None


tokenizer, model = get_model()

if tokenizer is None or model is None:
    st.stop()  # åœæ­¢è¿è¡Œï¼Œå¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥

with st.sidebar:
    st.markdown("## InternLM LLM")
    st.markdown("[InternLM](https://github.com/InternLM/InternLM.git)")
    st.markdown("[ChatAIåè¯ˆéª—](https://gitee.com/xiangboit/chat-ai-anti-fraud)")
    max_length = st.slider("max_length", 0, 1024, 512, step=1)
    system_prompt = st.text_input("System_Prompt",
                                  "ç°åœ¨ä½ è¦æ‰®æ¼”é˜²è¯ˆéª—ä¸“å®¶å¹¶ä¸”å’Œç”¨æˆ·è¿›è¡ŒèŠå¤©ï¼Œè¦æ±‚ç”¨æˆ·æä¾›ç›¸å…³çš„ä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯åˆ¤å®šç”¨æˆ·æ˜¯å¦é­å—äº†è¯ˆéª—å¹¶ç»™å‡ºåç»­å»ºè®®")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state.messages:
    st.chat_message("user").write(msg[0])
    st.chat_message("assistant").write(msg[1])

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    try:
        logging.info(f"User input received: {prompt}")

        # æ‹¼æ¥ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·è¾“å…¥
        full_prompt = f"{system_prompt}\n{prompt}"

        response, history = model.chat(tokenizer, full_prompt, history=st.session_state.messages)
        logging.info(f"Model response generated: {response}")

        st.session_state.messages.append((prompt, response))
        st.chat_message("assistant").write(response)
    except Exception as e:
        logging.error(f"Error during model response generation: {e}")
        st.error(f"Error during model response generation: {e}")