import streamlit as st
import logging
# æ¨¡å‹ä¸‹è½½
from modelscope import AutoTokenizer, AutoModelForCausalLM, snapshot_download
import torch

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.DEBUG)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ InternLM2-Chat-7B é˜²è¯ˆéª—ä¸“å®¶")
st.caption("ğŸš€ A streamlit chatbot powered by InternLM2 QLora")

try:
    logging.debug("Starting model download...")
    model_dir = snapshot_download("Shanghai_AI_Laboratory/internlm2-20b",revision='v1.1.0')
    logging.debug(f"Model downloaded to {model_dir}")
except Exception as e:
    logging.error(f"Error during model download: {e}")
    st.error(f"Error during model download: {e}")

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    try:
        logging.debug("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
        logging.debug("Tokenizer loaded successfully")

        logging.debug("Loading model...")
        model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16,offload_folder="/path/to/offload/folder")
        model = model.eval()
        logging.debug("Model loaded successfully")
        return tokenizer, model
    except Exception as e:
        logging.error(f"Error during model loading: {e}")
        st.error(f"Error during model loading: {e}")
        return None, None

tokenizer, model = get_model()

if tokenizer is None or model is None:
    st.stop()  # åœæ­¢è¿è¡Œï¼Œå¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥

with st.sidebar:
    st.markdown("## InternLM LLM")
    "[InternLM](https://github.com/InternLM/InternLM.git)"
    "[ChatAIåè¯ˆéª—](https://gitee.com/xiangboit/chat-ai-anti-fraud)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)
    system_prompt = st.text_input("System_Prompt", "ç°åœ¨ä½ è¦æ‰®æ¼”é˜²è¯ˆéª—ä¸“å®¶å¹¶ä¸”å’Œç”¨æˆ·è¿›è¡ŒèŠå¤©ï¼Œè¦æ±‚ç”¨æˆ·æä¾›ç›¸å…³çš„ä¿¡æ¯ï¼Œæ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯åˆ¤å®šç”¨æˆ·æ˜¯å¦é­å—äº†è¯ˆéª—å¹¶ç»™å‡ºåç»­å»ºè®®")

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message("user").write(msg[0])
    st.chat_message("assistant").write(msg[1])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    try:
        # æ„å»ºè¾“å…¥
        logging.info(f"User input received: {prompt}")
        response, history = model.chat(tokenizer, prompt, meta_instruction=system_prompt, history=st.session_state.messages)
        # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
        st.session_state.messages.append((prompt, response))
        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(response)
        logging.info(f"Model response generated and displayed: {response}")
    except Exception as e:
        logging.info(f"Error during model response generation: {e}")
        st.error(f"Error during model response generation: {e}")
